{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Editing the data\n",
    "___\n",
    "\n",
    "This file has been created to process the data file,  \"videogames.json\", and perform some basic operations. Also, his functions needs to be called before any other function in this script, Creates metadata and preprocesses the dataset in order to recover the information for the app.\n",
    "\n",
    "### Loading the original dataset\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20803\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "Videogames =\"\"\n",
    "with open('../../data/videogames.json','r') as data:\n",
    "    for letter in data:    \n",
    "        Videogames+=letter\n",
    "Videogames=json.loads(Videogames)\n",
    "print(len(Videogames))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing the missing plot\n",
    "---\n",
    "\n",
    "Some videogames doesn't have plots, instead, has the text: 'Add a Plot'. This block remove this text from all the games who has it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8874\n",
      "0\n",
      "8874\n"
     ]
    }
   ],
   "source": [
    "print(len([a for a in Videogames if a['plot']=='Add a Plot']))\n",
    "\n",
    "for a in Videogames:\n",
    "    if a['plot'] == 'Add a Plot':\n",
    "        a['plot']=''\n",
    "\n",
    "print(len([a for a in Videogames if a['plot']=='Add a Plot']))\n",
    "print(len([a for a in Videogames if a['plot']=='']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing the continous text from the long plots\n",
    "---\n",
    "\n",
    "Some plots are to large, and contains the text 'See full summary »' at the end of the plot. This is not ideal for presentation purposes. This block remove this messege"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "errors=set()\n",
    "\n",
    "for game in Videogames:\n",
    "    game['plot']=game['plot'].replace('See full summary »','')\n",
    "for game in Videogames:\n",
    "    if game['plot'].__contains__('See full summary »'):\n",
    "        errors.add(game['plot'])\n",
    "\n",
    "print(len(errors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the edited dataset\n",
    "---\n",
    "\n",
    "Create a new json witdh the information edited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/videogames_edited.json','w') as file:\n",
    "    json.dump(Videogames,file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the vocabulary\n",
    "---\n",
    "\n",
    "### Declaring auxiliar methods\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import gensim\n",
    "\n",
    "tokenized_docs = []\n",
    "vector_repr = []\n",
    "dictionary = {}\n",
    "vocabulary = []\n",
    "\n",
    "def tokenization_nltk(texts):\n",
    "    return [nltk.tokenize.word_tokenize(doc) for doc in texts]\n",
    "\n",
    "def remove_noise_nltk(tokenized_docs):\n",
    "    return [[word.lower() for word in doc if word.isalpha()] for doc in tokenized_docs]\n",
    "\n",
    "def remove_stopwords(tokenized_docs):\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    return [\n",
    "        [word for word in doc if word not in stop_words] for doc in tokenized_docs\n",
    "    ]\n",
    "\n",
    "def morphological_reduction_nltk(tokenized_docs, use_lemmatization=True):\n",
    "    if use_lemmatization:\n",
    "        lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "        return [\n",
    "            [lemmatizer.lemmatize(word) for word in doc]\n",
    "            for doc in tokenized_docs\n",
    "        ]\n",
    "    else:\n",
    "        stemmer = nltk.stem.PorterStemmer()\n",
    "        return [\n",
    "            [stemmer.stem(word) for word in doc] for doc in tokenized_docs\n",
    "        ]\n",
    "\n",
    "def filter_tokens_by_occurrence(tokenized_docs, no_below=5, no_above=0.5):\n",
    "    global dictionary\n",
    "    dictionary = gensim.corpora.Dictionary(tokenized_docs)\n",
    "    dictionary.filter_extremes(no_below=no_below, no_above=no_above)\n",
    "\n",
    "    filtered_words = [word for _, word in dictionary.iteritems()]\n",
    "    return [\n",
    "        [word for word in doc if word in filtered_words]\n",
    "        for doc in tokenized_docs\n",
    "    ]\n",
    "\n",
    "def vector_representation(tokenized_docs, dictionary, vector_repr, use_bow=True):\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in tokenized_docs]\n",
    "\n",
    "    if use_bow:\n",
    "        vector_repr = corpus\n",
    "    else:\n",
    "        tfidf = gensim.models.TfidfModel(corpus)\n",
    "        vector_repr = [tfidf[doc] for doc in corpus]\n",
    "\n",
    "    return vector_repr\n",
    "\n",
    "def pos_tagger_nltk(tokenized_docs):\n",
    "    return [nltk.pos_tag(doc) for doc in tokenized_docs]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the tokens, the vocabulary, the representative vector and the tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts=['\"' + a['name'] + '\" : ' + a['plot'] for a in Videogames]\n",
    "tokenized_docs= filter_tokens_by_occurrence(morphological_reduction_nltk(\n",
    "    remove_stopwords(remove_noise_nltk(tokenization_nltk(texts)))))\n",
    "vocabulary = list(dictionary.token2id.keys())\n",
    "vector_repr = vector_representation(tokenized_docs, dictionary, vector_repr,False)\n",
    "pos_tags = pos_tagger_nltk(tokenized_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/pos_tags.json','w') as file:\n",
    "    json.dump(pos_tags,file)\n",
    "\n",
    "with open('../../data/vector_repr.json','w') as file:\n",
    "    json.dump(vector_repr,file)\n",
    "\n",
    "with open('../../data/vocabulary.json','w') as file:\n",
    "    json.dump(vocabulary,file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
